{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage Mix Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you pay any attention to pop music nowadays, you probably have noticed the larger precense of Kpop. Worldwide, Kpop is gaining popularity out of Asia. Most notebly, BTS has had multiple albums in the Billboard World Albums chart since last summer in addition to making the rounds on late night television. Another popular group, Blackpink, performed at Coachella. \n",
    "\n",
    "\n",
    "I personally really enjoy Kpop. I'm going to try and defend that possition as I have failed a great many times before, but at least now with it's popularity, I know I'm not alone\n",
    "\n",
    "So where does a programming project fit into all of this? Well, there's a paticular part of Kpop that reliably occurs and any time there's a reccurance, there's always a cool project.\n",
    "\n",
    "Essentially, the mainstream Kpop industry promotes their groups in cycles. I'm no expert but here's how I understand it. (maybe just include an exerpt from a good article about it). Groups will first build hype for a new single or album by releasing teaser images and videos. Then, once the song or album is dropped, the fans will start streaming and the group will already have multiple performances lined up. However, these aren't like traditional tours. The groups have to perform their song at well know events like Inkigayo or M!Countdown, which occur regularly as well. Their performances from each of these shows is now almost guaranteed to be uploaded to YouTube.\n",
    "\n",
    "Given that the correography for the song does not change, the wealth of footage makes for great montages. This has created a whole new community on YouTube where users will create \"stage mixes\" of the different performances while a group was promoting.\n",
    "\n",
    "At a certain point, after being a avid comsumer of this content, I thought to myself, these mixes definitely could be done automatically. All you had to do was align the audio and make the cuts. What could possibly be hard about that? I find the answer to my question because the day after I set out to see if I could do it.\n",
    "\n",
    "It took a while to work through many bugs and bag logic (I swear I'm good at programming in usually), but I eventually succeeded. Meet Stage Mix Generator (SMG) 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "The basic algorithm for SMG 1.0 is as follows:\n",
    "\n",
    "1. Retrieve stage videos and audios\n",
    "2. Retrieve song audio\n",
    "3. Align stage videos to audios\n",
    "4. Find cuts in stage videos\n",
    "4. Create mix using vuts\n",
    "5. Render mix video\n",
    "6. Upload to YouTube\n",
    "\n",
    "The most interesting parts of the algorithm come with aligning the stage videos with the song audio and cutting the mix to match the stage videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aligning Stage Videos to Song Audio\n",
    "\n",
    "One thing that I believe I've mentioned before in my blog posts is that one of the most crucial skills a programmer should have is an intuitive sense of when a problem likely has implemented solutions and how to find them. Given the generality of my problem and the likelyhood a solution would be useful in many fields, I started to search. Right away, I found this:\n",
    "\n",
    "![png](https://github.com/allisonnicoledeal/VideoSync/raw/master/screenshots/screenshot.png)\n",
    "\n",
    "Well you don't say! This is almost exactly what I'm already trying to do. [VideoSync]() is a project by [Allison Deal]() which \"automatically synchronizes and combines personal and crowd-sourced YouTube video clips to recreate a live concert experience from multiple angles.\"\n",
    "\n",
    "Here's a description of the program from the repo's [README]():\n",
    "> * **YouTube Link**: Download YouTube videos as MP4 files with youtube-dl command line program.\n",
    "> * **WAV File**: Strip audio from video file using the avconv audio/video converter. Read audio data using the Python scipy library.\n",
    "> * **Fourier Transform of Audio Signal**: Split audio into bins and apply the Fourier transform on each bin using the numpy library. The Fourier transform converts each bin data from the time domain to the frequency domain.\n",
    "> * **Peak Frequencies**: Identify the frequency with the highest intensity in each bin to create a peak frequency constellation.\n",
    "> * **Frequency Constellation Alignment**: Determine time offset by aligning frequency constellations of the two audio files.\n",
    "\n",
    "This is super close to what I was trying to do, however with a few subtle differences:\n",
    "\n",
    "1. It downloads the videos in MP4 format which with YouTube limits you to 720p\n",
    "2. It's designed to sync videos of the same performance\n",
    "3. The performances are meant to be entire concerts, not single songs\n",
    "4. The final video is not edited\n",
    "\n",
    "The key difference here is the final product. Deal's goal was to give a live concert experience by showing multiple angles at the same time. I was looking to edit different stage performances into one video, similar to the stage mixes on YouTube. Even with these differences, I knew I could use the underlying code that aligned the videos. Unfortunately, VideoSync has not been updated since 2014 so I struggled to get any of the python to run without errors. After some more googling, I found that the main function in the VideoSync code was adapted to be used in the [cvcalib]() library. Since this library had been updated more recently, the code worked out of the box. If anyone is curious, the function I use can be located [here](https://github.com/Algomorph/cvcalib/blob/4ed638ea523b6d1059556a135576c7afa3a4b07f/audiosync/offset.py#L172).\n",
    "\n",
    "So now that I could align the videos, I needed to edit them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cutting Mix to Match Video Clip Cuts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum Viable Product: Cut every 5 seconds\n",
    "\n",
    "Finding a good library to edit the mixes was actually a lot easier than I expected. Through a quick search, I found [MoviePy](https://github.com/Zulko/moviepy). Here's an excerpt the description from the repository:\n",
    "\n",
    "> MoviePy (full [documentation](http://zulko.github.io/moviepy/)) is a Python library for video editing: cutting, concatenations, title insertions, video compositing (a.k.a. non-linear editing), video processing, and creation of custom effects.\n",
    "\n",
    "Using the library, I was easily able to make a video that was cut every 5 seconds. Below is an example video that the algorithm created using the code thus far. Note I had not yet built the upload to YouTube feature.\n",
    "\n",
    "\n",
    "<iframe src=\"https://www.youtube.com/embed/hrnOZr59vLw\" frameborder=\"0\" allow=\"accelerometer; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "\n",
    "\n",
    "Pretty cool, right? I sure thought so. This was definitely my minimum viable product (MVP). I now had a working prototype of the system I envisioned earlier. However, I knew I could do better and so I set out to find a method of detecting the cuts in the stage performance videos.\n",
    "\n",
    "### Stretch Goal: Detect and match cuts of stage videos used\n",
    "\n",
    "Although I quickly found a library that could detect cuts in videos, installing OpenCV on Windows turned into a nightmare and delayed my completion of the project rather significantly. Nonetheless, here's how I did it.\n",
    "\n",
    "The library I used is called [PySceneDetect](https://github.com/Breakthrough/PySceneDetect). It provides two main algorithms for detecting the cuts in videos. The Content Detector \"compares the difference in content between adjacent frames against a set threshold/score, which if exceeded, triggers a scene cut.\" The Threshold Detector \"uses a set intensity level to detect scene cuts when the average frame intensity passes the set threshold.\" The Content Detector algorithm is rather self-explanatory but the Threshold Detector algorithm is a little more nuanced. Essentially, it is mostly good for finding cuts where the video fades to black. Since none of the cuts in the stage videos were fades, I needed to use the Content Detector. More information about these algorithms can be found in [their documentation](https://pyscenedetect-manual.readthedocs.io/en/latest/cli/detectors.html).\n",
    "\n",
    "\n",
    "To determine the threshold at which the algo would register a cut, I followed the instructions in the documentation:\n",
    "\n",
    "> The optimal threshold can be determined by generating a stats file (-s), opening it with a spreadsheet editor (e.g. Excel), and examining the content_val column. This value should be very small between similar frames, and grow large when a big change in content is noticed (look at the values near frame numbers/times where you know a scene change occurs). The threshold value should be set so that most scenes fall below the threshold value, and scenes where changes occur should exceed the threshold value (thus triggering a scene change).\n",
    "\n",
    "For the stage videos, the default value of 30 was a bit too high so I chose to use 40. From this point, my implementation is quite simple. First, I find all the cuts in each video to be used in the mix. Then, I make the cuts by repeatedly chosing a random video and finding next cut from the current timestamp the loop is at. This algorithm is clearly $O(n^2)$ but its efficiency should be fine for now. Below is an example video that was edited using this method.\n",
    "\n",
    "<iframe src=\"https://www.youtube.com/embed/hrnOZr59vLw\" frameborder=\"0\" allow=\"accelerometer; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "\n",
    "Now that the resulting stage mix was satisfactory (at least to me), the final step was to automatically share my creation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading to YouTube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ended up being the easiest part. Although, [Tokland](https://github.com/tokland)'s [youtube-upload](https://github.com/tokland/youtube-upload) doesn't provide an easily importable module, it does have a command line tool that can be run using python using the [subprocess](https://docs.python.org/3/library/subprocess.html) built-in library. \n",
    "\n",
    "Before using, I followed the instructions in the [setup](https://github.com/tokland/youtube-upload#setup) section of the README. Once the correct configuration files were in place, I created a small wrapper and setup the upload to trigger after the mix is rendered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "###  Prerequisites\n",
    "\n",
    "- [OpenCV](https://opencv.org/)\n",
    "- [FFmpeg](https://ffmpeg.org/)\n",
    "\n",
    "### Clone Repository\n",
    "\n",
    "`git clone --recurse-submodules https://github.com/seangtkelley/stage-mix-generator`\n",
    "\n",
    "### Python Requirements\n",
    "\n",
    "#### Mac and Linux\n",
    "\n",
    "`pip install requirements.txt`\n",
    "\n",
    "#### Windows \n",
    "\n",
    "`conda env create -f requirements.yml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to VideoSync, I also created a flask app, although mine is much less ornate.\n",
    "\n",
    "#### Mac and Linux\n",
    "\n",
    "```sh\n",
    "export FLASK_APP=web_ui.py\n",
    "flask run\n",
    "```\n",
    "\n",
    "#### Windows PowerShell\n",
    "\n",
    "```powershell\n",
    "$env:FLASK_APP = \"hello.py\"\n",
    "flask run\n",
    "```\n",
    "\n",
    "Here are some screenshots of the app. I think the form should be relatively self-explanatory.\n",
    "\n",
    "![alt text][logo]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- include final product video\n",
    "\n",
    "- will be contributing to Allison's work soon\n",
    "- conclusion is that programming can be applied to almost anything\n",
    "    - be creative\n",
    "    - CS can combine with almost any of your interests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
