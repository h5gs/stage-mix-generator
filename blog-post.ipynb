{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage Mix Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you pay any attention to pop music nowadays, you probably have noticed the larger precense of Kpop. Worldwide, Kpop is gaining popularity out of Asia. Most notebly, BTS has had multiple albums in the Billboard World Albums chart since last summer in addition to making the rounds on late night television. Another popular group, Blackpink, performed at Coachella. \n",
    "\n",
    "\n",
    "I personally really enjoy Kpop. I'm going to try and defend that possition as I have failed a great many times before, but at least now with it's popularity, I know I'm not alone\n",
    "\n",
    "So where does a programming project fit into all of this? Well, there's a paticular part of Kpop that reliably occurs and any time there's a reccurance, there's always a cool project.\n",
    "\n",
    "Essentially, the mainstream Kpop industry promotes their groups in cycles. I'm no expert but here's how I understand it. (maybe just include an exerpt from a good article about it). Groups will first build hype for a new single or album by releasing teaser images and videos. Then, once the song or album is dropped, the fans will start streaming and the group will already have multiple performances lined up. However, these aren't like traditional tours. The groups have to perform their song at well know events like Inkigayo or M!Countdown, which occur regularly as well. Their performances from each of these shows is now almost guaranteed to be uploaded to YouTube.\n",
    "\n",
    "Given that the correography for the song does not change, the wealth of footage makes for great montages. This has created a whole new community on YouTube where users will create \"stage mixes\" of the different performances while a group was promoting.\n",
    "\n",
    "At a certain point, after being a avid comsumer of this content, I thought to myself, these mixes definitely could be done automatically. All you had to do was align the audio and make the cuts. What could possibly be hard about that? I find the answer to my question because the day after I set out to see if I could do it.\n",
    "\n",
    "It took a while to work through many bugs and bag logic (I swear I'm good at programming in usually), but I eventually succeeded. Meet Stage Mix Generator (SMG) 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "The basic algorithm for SMG 1.0 is as follows:\n",
    "\n",
    "1. Retrieve stage videos and audios\n",
    "2. Retrieve song audio\n",
    "3. Align stage videos to audios\n",
    "4. Find cuts in stage videos\n",
    "4. Create mix using vuts\n",
    "5. Render mix video\n",
    "6. Upload to YouTube\n",
    "\n",
    "The most interesting parts of the algorithm come with aligning the stage videos with the song audio and making the cuts to match the videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aligning Stage Videos to Song Audio\n",
    "\n",
    "One thing that I believe I've mentioned before in my blog posts is that one of the most crucial skills a programmer should have is an intuitive sense of when a problem likely has implemented solutions and how to find them. Given the generality of my problem, aligning videos with audio, and the likelyhood a solution would be useful in many fields, I started to search. Right away, I found this:\n",
    "\n",
    "![png](https://github.com/allisonnicoledeal/VideoSync/raw/master/screenshots/screenshot.png)\n",
    "\n",
    "Well you don't say! This is almost exactly what I'm already trying to do. [VideoSync]() is a project by [Allison Deal]() which \"automatically synchronizes and combines personal and crowd-sourced YouTube video clips to recreate a live concert experience from multiple angles.\"\n",
    "\n",
    "Here's a description of the program from the repo's [README]():\n",
    "> * **YouTube Link**: Download YouTube videos as MP4 files with youtube-dl command line program.\n",
    "> * **WAV File**: Strip audio from video file using the avconv audio/video converter. Read audio data using the Python scipy library.\n",
    "> * **Fourier Transform of Audio Signal**: Split audio into bins and apply the Fourier transform on each bin using the numpy library. The Fourier transform converts each bin data from the time domain to the frequency domain.\n",
    "> * **Peak Frequencies**: Identify the frequency with the highest intensity in each bin to create a peak frequency constellation.\n",
    "> * **Frequency Constellation Alignment**: Determine time offset by aligning frequency constellations of the two audio files.\n",
    "\n",
    "This is super close to what I was trying to do, however with a few subtle differences:\n",
    "\n",
    "1. It downloads the videos in MP4 format which with YouTube limits you to 720p\n",
    "2. It's designed to sync videos of the same performance\n",
    "3. The performances are meant to be entire concerts, not single songs\n",
    "4. The final video is not edited\n",
    "\n",
    "The key difference here is the final product. Deal's goal was to give a live concert experience by showing multiple angles at the same time. I was looking to edit the videos into one, similar to the stage mixes on YouTube. Even with these differences, I knew I could use the underlying code that aligned the videos. Unfortunately, VideoSync has not been updated since 2014 so I struggled to get any of the python to run without errors. After some more googling, I found that the main function in the VideoSync code was adapted to be used in the [cvcalib]() library. Since this library had been updated more recently, the code worked out of the box. If anyone is curious, the function I use can be located [here](https://github.com/Algomorph/cvcalib/blob/4ed638ea523b6d1059556a135576c7afa3a4b07f/audiosync/offset.py#L172).\n",
    "\n",
    "So now that I could align the videos, I needed to edit them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cutting Mix to Match Video Clip Cuts\n",
    "\n",
    "- implementation heavy (lots of different moving parts)\n",
    "- moviepy to do the editing\n",
    "- first started with only making cuts every 5 seconds\n",
    "    - maybe show an example video\n",
    "- wanted to make something more dynamic\n",
    "- PySceneDetect to find cuts\n",
    "    - threshold detection is not what were looking for because none of the cuts in the stage performances go to black\n",
    "    - content detection works!\n",
    "        - determines cuts by measuring difference between each pair of frames\n",
    "    - (maybe put in description of algos from readme)\n",
    "    - used stats file to find ideal threshold for stage videos\n",
    "    - find all cuts in each video\n",
    "    - make cuts by chosing random video and finding next cut from current timestamp\n",
    "        - (mention there probably could be a better algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum Viable Product: Cut every 5 seconds\n",
    "\n",
    "Finding a good library to edit the mixes was actually a lot easier than I expected. Through a quick search, I found [MoviePy](https://github.com/Zulko/moviepy). Here's an excerpt the description from the repository:\n",
    "\n",
    "> MoviePy (full [documentation](http://zulko.github.io/moviepy/)) is a Python library for video editing: cutting, concatenations, title insertions, video compositing (a.k.a. non-linear editing), video processing, and creation of custom effects.\n",
    "\n",
    "Using the library, I was easily able to make a video that was cut every 5 seconds. Below is an example video that the algorithm created using the code thus far. Note I had not yet built the upload to YouTube feature.\n",
    "\n",
    "\n",
    "<iframe src=\"https://www.youtube.com/embed/hrnOZr59vLw\" frameborder=\"0\" allow=\"accelerometer; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "\n",
    "\n",
    "Pretty cool, right? I thought so for sure. This was definitely my minimum viable product (MVP). I now had a working prototype of system I envisioned earlier. However, I knew I could do better and so I set out to find a way to find and match the cuts from the stage performance videos.\n",
    "\n",
    "### Stretch Goal: Detect and match cuts of stage videos used\n",
    "\n",
    "Although I quickly found a library that could detect cuts in videos, installing OpenCV on Windows turned into a nightmare and delayed my completion of the project rather significantly. Nonetheless, here's how I did it.\n",
    "\n",
    "The library I used is called [PySceneDetect](). It provides two main algorithms for detecting the cuts in videos. The Content Detector \"compares the difference in content between adjacent frames against a set threshold/score, which if exceeded, triggers a scene cut.\" The Threshold Detector \"uses a set intensity level to detect scene cuts when the average frame intensity passes the set threshold.\" The Content Detector algorithm is rather self-explanatory but the Threshold Detector algorithm is a little more nuanced. Essentially, it is mostly good for finding cuts where the video fades to black. Since none of the cuts in the stage videos were fades, I needed to use the Content Detector. \n",
    "\n",
    "\n",
    "To determine the threshold at which the algo would register a cut, I followed the instructions in the documentation.\n",
    "\n",
    "> put the quote or maybe your explanation here\n",
    "\n",
    "Using the threshold value, I \n",
    "- find all cuts in each video\n",
    "- make cuts by chosing random video and finding next cut from current timestamp\n",
    "    - (mention there probably could be a better algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading to YouTube\n",
    "\n",
    "- tokland's youtube-upload library\n",
    "- basically follow the instructions, get all the keys, put them in the right place, it works\n",
    "- use the title and artist input and simply upload the video afte rendering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "- install opencv\n",
    "- install ffmpeg\n",
    "\n",
    "Finally, install the python requirements.\n",
    "\n",
    "#### Mac and Linux\n",
    "\n",
    "`pip install requirements.txt`\n",
    "\n",
    "#### Windows \n",
    "\n",
    "`conda install somethingggggg`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "- similar to VideoSync, I made a flask app\n",
    "- run it with \n",
    "    - different instructions for making the env var\n",
    "    - python web_ui.py\n",
    "- include screenshot with example inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to VideoSync, I also created a flask app, although mine is much less ornate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- include final product video\n",
    "\n",
    "- will be contributing to Allison's work soon\n",
    "- conclusion is that programming can be applied to almost anything\n",
    "    - be creative\n",
    "    - CS can combine with almost any of your interests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
